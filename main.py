#!/usr/bin/env python3
"""
Deep Research Clone - æ”¹å–„ç‰ˆ
è¨€èªãƒ¢ãƒ‡ãƒ«ã¨Webæ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ãŸç ”ç©¶æ”¯æ´ãƒ„ãƒ¼ãƒ«
ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹è¿½åŠ æ¤œç´¢æ©Ÿèƒ½ä»˜ã
"""

import os
import json
import requests
from typing import List, Dict, Optional, Set
from dataclasses import dataclass
from pathlib import Path
import markdown
from dotenv import load_dotenv
import re
import yaml
from datetime import datetime
import time
import random

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

@dataclass
class SearchResult:
    """æ¤œç´¢çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    title: str
    url: str
    snippet: str
    search_query: str  # ã©ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã§å–å¾—ã•ã‚ŒãŸã‹
    date_info: Optional[str] = None  # æ—¥ä»˜æƒ…å ±
    reliability_score: float = 1.0  # ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
    source_type: str = "unknown"  # æƒ…å ±æºã‚¿ã‚¤ãƒ—ï¼ˆofficial, news, academic, blog, etc.ï¼‰

@dataclass
class Citation:
    """å¼•ç”¨æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    source_title: str
    source_url: str
    content: str
    search_query: str
    relevance_score: float = 1.0
    date_info: Optional[str] = None  # æ—¥ä»˜æƒ…å ±

@dataclass
class ResearchResult:
    """ç ”ç©¶çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    query: str
    search_results: List[SearchResult]
    analysis: str
    summary: str
    citations: List[Citation]
    additional_queries: List[str]
    final_report: str

class Config:
    """è¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config_path: str = "config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()

    def _load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                # ç’°å¢ƒå¤‰æ•°ã‚’å±•é–‹
                self._expand_env_vars(config)
                return config
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            return {
                'language_model': {
                    'default': 'ollama',
                    'ollama': {'model_name': 'llama2', 'base_url': 'http://localhost:11434'},
                    'openai': {'model': 'gpt-3.5-turbo', 'max_tokens': 2000, 'temperature': 0.7},
                    'gemini': {'model': 'gemini-2.0-flash', 'temperature': 0.7}
                },
                'iteration': {
                    'max_iterations': 3,
                    'initial_search_count': 8,
                    'additional_search_count': 5,
                    'max_additional_queries': 3
                },
                'search': {
                    'engine': 'google',
                    'max_results': 5,
                    'google': {
                        'api_key': os.getenv('GOOGLE_SEARCH_API_KEY'),
                        'search_engine_id': os.getenv('GOOGLE_SEARCH_ENGINE_ID')
                    }
                },
                'citations': {
                    'auto_extract': True,
                    'relevance_threshold': 0.5,
                    'format': 'numbered'
                },
                'output': {
                    'format': 'markdown',
                    'directory': './output',
                    'filename_template': 'research_{query}_{timestamp}',
                    'markdown': {
                        'include_search_results': True,
                        'include_analysis': True,
                        'include_summary': True,
                        'include_final_report': True,
                        'include_citations': True,
                        'include_search_history': True
                    }
                }
            }

    def _expand_env_vars(self, obj):
        """è¾æ›¸å†…ã®ç’°å¢ƒå¤‰æ•°ã‚’å±•é–‹"""
        if isinstance(obj, dict):
            for key, value in obj.items():
                if isinstance(value, str) and value.startswith('${') and value.endswith('}'):
                    # ${VAR_NAME} å½¢å¼ã®ç’°å¢ƒå¤‰æ•°ã‚’å±•é–‹
                    env_var = value[2:-1]  # ${} ã‚’é™¤å»
                    obj[key] = os.getenv(env_var, value)
                elif isinstance(value, (dict, list)):
                    self._expand_env_vars(value)
        elif isinstance(obj, list):
            for item in obj:
                if isinstance(item, (dict, list)):
                    self._expand_env_vars(item)

    def get(self, key: str, default=None):
        """è¨­å®šå€¤ã‚’å–å¾—"""
        keys = key.split('.')
        value = self.config
        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default
        return value

class LanguageModel:
    """è¨€èªãƒ¢ãƒ‡ãƒ«ã®æŠ½è±¡ã‚¯ãƒ©ã‚¹"""

    def generate(self, prompt: str) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        raise NotImplementedError

class OllamaModel(LanguageModel):
    """Ollamaãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, config: Config):
        self.model_name = config.get('language_model.ollama.model_name', 'llama2')
        self.base_url = config.get('language_model.ollama.base_url', 'http://localhost:11434')

    def generate(self, prompt: str) -> str:
        """Ollama APIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=60  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’60ç§’ã«è¨­å®š
            )
            response.raise_for_status()
            return response.json()["response"]
        except requests.exceptions.Timeout:
            return f"Ollama API ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ãƒ¢ãƒ‡ãƒ« {self.model_name} ã®å¿œç­”ãŒ60ç§’ã‚’è¶…ãˆã¾ã—ãŸ"
        except requests.exceptions.ConnectionError:
            return f"Ollama API æ¥ç¶šã‚¨ãƒ©ãƒ¼: {self.base_url} ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„"
        except requests.exceptions.RequestException as e:
            return f"Ollama API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}"
        except Exception as e:
            return f"Ollama API äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}"

class OpenAIModel(LanguageModel):
    """OpenAI APIãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, config: Config):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model = config.get('language_model.openai.model', 'gpt-3.5-turbo')
        self.max_tokens = config.get('language_model.openai.max_tokens', 2000)
        self.temperature = config.get('language_model.openai.temperature', 0.7)

        if not self.api_key:
            raise ValueError("OpenAI API ã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

    def generate(self, prompt: str) -> str:
        """OpenAI APIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            import openai
            client = openai.OpenAI(api_key=self.api_key)

            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"OpenAI API ã‚¨ãƒ©ãƒ¼: {e}"

class GoogleGeminiModel(LanguageModel):
    """Google Gemini APIãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, config: Config):
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.model = config.get('language_model.gemini.model', 'gemini-2.0-flash')
        self.temperature = config.get('language_model.gemini.temperature', 0.7)

        if not self.api_key:
            raise ValueError("Google API ã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

    def generate(self, prompt: str) -> str:
        """Google Gemini APIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            model = genai.GenerativeModel(self.model)
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"Google Gemini API ã‚¨ãƒ©ãƒ¼: {e}"

class WebSearcher:
    """Webæ¤œç´¢ã‚¯ãƒ©ã‚¹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""

    def __init__(self, config: Config):
        self.config = config
        self.api_key = config.get('search.google.api_key')
        self.search_engine_id = config.get('search.google.search_engine_id')
        self.max_results = config.get('search.max_results', 5)

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®šï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        self.requests_per_second = config.get('search.rate_limit.requests_per_second', 8)
        self.max_retries = config.get('search.rate_limit.max_retries', 3)
        self.retry_delay_base = config.get('search.rate_limit.retry_delay_base', 2)
        self.last_request_time = 0

    def _extract_date_info(self, title: str, snippet: str) -> Optional[str]:
        """ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ã‚‰æ—¥ä»˜æƒ…å ±ã‚’æŠ½å‡º"""
        # æ—¢å­˜ã®æ—¥ä»˜æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
            r'(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})å¹´(\d{1,2})æœˆ',
            r'(\d{4})å¹´',
            r'(\d{1,2})æ™‚é–“å‰',
            r'(\d{1,2})æ—¥å‰',
            r'(\d{1,2})é€±é–“å‰',
            r'(\d{1,2})ãƒ¶æœˆå‰',
            r'(\d{1,2})å¹´å‰',
        ]

        for pattern in date_patterns:
            match = re.search(pattern, title + " " + snippet)
            if match:
                return match.group(0)

        return None

    def _evaluate_source_reliability(self, url: str, title: str, snippet: str) -> Dict[str, any]:
        """æƒ…å ±æºã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡"""
        reliability_score = 1.0
        source_type = "unknown"

        # URLãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡
        url_lower = url.lower()

        # å…¬å¼ã‚µã‚¤ãƒˆãƒ»æ”¿åºœæ©Ÿé–¢
        official_domains = [
            'gov.jp', 'go.jp', 'pref.', 'city.', 'town.', 'village.',
            'ac.jp', 'edu.', 'university.', 'college.',
            'org', 'association.', 'foundation.'
        ]

        # ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢
        news_domains = [
            'nhk.', 'asahi.', 'mainichi.', 'yomiuri.', 'sankei.',
            'nikkei.', 'reuters.', 'bloomberg.', 'cnn.', 'bbc.'
        ]

        # å­¦è¡“ãƒ»ç ”ç©¶æ©Ÿé–¢
        academic_domains = [
            'research.', 'study.', 'journal.', 'paper.', 'arxiv.',
            'pubmed.', 'scholar.google.', 'jstor.'
        ]

        # ãƒ–ãƒ­ã‚°ãƒ»å€‹äººã‚µã‚¤ãƒˆ
        blog_indicators = [
            'blog.', 'note.com', 'hatena.', 'ameblo.', 'fc2.',
            'wordpress.', 'tumblr.', 'medium.'
        ]

        # ä¿¡é ¼æ€§è©•ä¾¡
        if any(domain in url_lower for domain in official_domains):
            reliability_score = 0.9
            source_type = "official"
        elif any(domain in url_lower for domain in news_domains):
            reliability_score = 0.8
            source_type = "news"
        elif any(domain in url_lower for domain in academic_domains):
            reliability_score = 0.85
            source_type = "academic"
        elif any(indicator in url_lower for indicator in blog_indicators):
            reliability_score = 0.5
            source_type = "blog"
        else:
            reliability_score = 0.6
            source_type = "general"

        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ã‚¹ãƒ‹ãƒšãƒƒãƒˆãƒ™ãƒ¼ã‚¹ã®èª¿æ•´
        content_lower = (title + " " + snippet).lower()

        # ä¿¡é ¼æ€§ã‚’ä¸‹ã’ã‚‹è¦ç´ 
        if any(word in content_lower for word in ['å™‚', 'ãƒ‡ãƒ', 'æœªç¢ºèª', 'æ¨æ¸¬', 'æ†¶æ¸¬']):
            reliability_score *= 0.7

        # ä¿¡é ¼æ€§ã‚’ä¸Šã’ã‚‹è¦ç´ 
        if any(word in content_lower for word in ['ç™ºè¡¨', 'å…¬å¼', 'ç¢ºèª', 'èª¿æŸ»çµæœ', 'ãƒ‡ãƒ¼ã‚¿']):
            reliability_score *= 1.1

        # ã‚¹ã‚³ã‚¢ã‚’0.0-1.0ã®ç¯„å›²ã«åˆ¶é™
        reliability_score = max(0.0, min(1.0, reliability_score))

        return {
            "reliability_score": reliability_score,
            "source_type": source_type
        }

    def search(self, query: str, num_results: int = None) -> List[SearchResult]:
        """Webæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
        if num_results is None:
            num_results = self.config.get('search.max_results', 5)

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®é©ç”¨
        self._apply_rate_limit()

        for attempt in range(self.max_retries):
            try:
                # Google Custom Search APIã‚’ä½¿ç”¨
                api_key = self.config.get('search.google.api_key')
                search_engine_id = self.config.get('search.google.search_engine_id')

                if not api_key or not search_engine_id:
                    raise ValueError("Google Custom Search API ã‚­ãƒ¼ã¨æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³IDãŒå¿…è¦ã§ã™")

                url = "https://www.googleapis.com/customsearch/v1"
                params = {
                    'key': api_key,
                    'cx': search_engine_id,
                    'q': query,
                    'num': min(num_results, 10)  # Google APIã®æœ€å¤§å€¤ã¯10
                }

                response = requests.get(url, params=params)

                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å‡¦ç†
                if response.status_code == 429:
                    # Too Many Requests - ãƒªãƒˆãƒ©ã‚¤
                    wait_time = self.retry_delay_base * (2 ** attempt) + random.uniform(0, 1)
                    print(f"âš ï¸  ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚{wait_time:.1f}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(wait_time)
                    continue
                elif response.status_code == 403:
                    # Forbidden - APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼
                    raise ValueError("APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚Google Cloud Consoleã§è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                else:
                    response.raise_for_status()

                data = response.json()

                results = []
                if 'items' in data:
                    for item in data['items']:
                        title = item.get('title', '')
                        url = item.get('link', '')
                        snippet = item.get('snippet', '')

                        # æ—¥ä»˜æƒ…å ±ã‚’æŠ½å‡º
                        date_info = self._extract_date_info(title, snippet)

                        # ä¿¡é ¼æ€§è©•ä¾¡ã‚’å®Ÿè¡Œ
                        reliability_info = self._evaluate_source_reliability(url, title, snippet)

                        result = SearchResult(
                            title=title,
                            url=url,
                            snippet=snippet,
                            search_query=query,
                            date_info=date_info,
                            reliability_score=reliability_info["reliability_score"],
                            source_type=reliability_info["source_type"]
                        )
                        results.append(result)

                return results

            except requests.exceptions.RequestException as e:
                if attempt < self.max_retries - 1:
                    wait_time = self.retry_delay_base * (2 ** attempt) + random.uniform(0, 1)
                    print(f"âš ï¸  ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}. {wait_time:.1f}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                    return []
            except Exception as e:
                print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                return []

        print("âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸ")
        return []

    def _apply_rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é©ç”¨"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time

        if time_since_last_request < 1.0 / self.requests_per_second:
            sleep_time = (1.0 / self.requests_per_second) - time_since_last_request
            time.sleep(sleep_time)

        self.last_request_time = time.time()

class CitationManager:
    """å¼•ç”¨ç®¡ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Config):
        self.config = config
        self.citations: List[Citation] = []
        self.citation_counter = 1
        self.auto_extract = config.get('citations.auto_extract', True)
        self.relevance_threshold = config.get('citations.relevance_threshold', 0.5)

    def add_citation(self, source: SearchResult, content: str, relevance_score: float = 1.0) -> int:
        """å¼•ç”¨ã‚’è¿½åŠ """
        if relevance_score >= self.relevance_threshold:
            citation = Citation(
                source_title=source.title,
                source_url=source.url,
                content=content,
                search_query=source.search_query,
                relevance_score=relevance_score,
                date_info=source.date_info
            )
            self.citations.append(citation)
            return len(self.citations)
        return 0

    def get_citation_text(self, citation_index: int) -> str:
        """å¼•ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        if 0 <= citation_index - 1 < len(self.citations):
            citation = self.citations[citation_index - 1]
            return f"[{citation_index}] {citation.source_title} ({citation.source_url})"
        return f"[{citation_index}] å¼•ç”¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

    def get_all_citations(self) -> List[Citation]:
        """å…¨ã¦ã®å¼•ç”¨ã‚’å–å¾—"""
        return self.citations

class DeepResearch:
    """Deep Researchã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""

    def __init__(self, model_type: str = None, config_path: str = "config.yaml"):
        self.config = Config(config_path)
        self.model_type = model_type or self.config.get('language_model.default', 'ollama')
        self.model = self._create_model(self.model_type)
        self.review_model = self._create_model(self.model_type)  # ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®åˆ¥ãƒ¢ãƒ‡ãƒ«
        self.searcher = WebSearcher(self.config)
        self.citation_manager = CitationManager(self.config)
        self.all_search_results: List[SearchResult] = []

        # è¨­å®šã‹ã‚‰å€¤ã‚’å–å¾—
        self.max_iterations = self.config.get('iteration.max_iterations', 3)
        self.initial_search_count = self.config.get('iteration.initial_search_count', 8)
        self.additional_search_count = self.config.get('iteration.additional_search_count', 5)
        self.max_additional_queries = self.config.get('iteration.max_additional_queries', 3)

        # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
        self.today_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
        self.today_year = datetime.now().year
        self.today_month = datetime.now().month
        self.today_day = datetime.now().day

    def _create_model(self, model_type: str) -> LanguageModel:
        """æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ—ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
        if model_type == "ollama":
            return OllamaModel(self.config)
        elif model_type == "openai":
            return OpenAIModel(self.config)
        elif model_type == "gemini":
            return GoogleGeminiModel(self.config)
        else:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")

    def research(self, query: str, max_iterations: int = None) -> ResearchResult:
        """æŒ‡å®šã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§Deep Researchã‚’å®Ÿè¡Œï¼ˆåå¾©æ”¹å–„ç‰ˆï¼‰"""
        if max_iterations is None:
            max_iterations = self.max_iterations

        print(f"ğŸ” åˆæœŸæ¤œç´¢ä¸­: {query}")

        # åˆæœŸæ¤œç´¢
        initial_results = self.searcher.search(query, num_results=self.initial_search_count)

        # æ¤œç´¢çµæœãŒç©ºã®å ´åˆã®å‡¦ç†
        if not initial_results:
            print("âŒ æ¤œç´¢çµæœãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä»¥ä¸‹ã®åŸå› ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ï¼š")
            print("   - Google Custom Search APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ãŸ")
            print("   - APIã‚­ãƒ¼ã¾ãŸã¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³IDãŒç„¡åŠ¹")
            print("   - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã®å•é¡Œ")
            print("   - æ¤œç´¢ã‚¯ã‚¨ãƒªã«å•é¡ŒãŒã‚ã‚‹")
            print("\nå¯¾å‡¦æ³•ï¼š")
            print("   - ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
            print("   - .envãƒ•ã‚¡ã‚¤ãƒ«ã®APIè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            print("   - æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å¤‰æ›´ã—ã¦ã¿ã¦ãã ã•ã„")
            return None

        # ä¿¡é ¼æ€§ã«åŸºã¥ã„ã¦ä¸¦ã³æ›¿ãˆ
        initial_results = self._sort_results_by_reliability(initial_results)

        # ä¿¡é ¼æ€§ã®ä½ã„çµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        reliability_threshold = self.config.get('citations.reliability_threshold', 0.3)
        initial_results = self._filter_results_by_reliability(initial_results, reliability_threshold)

        self.all_search_results.extend(initial_results)

        # åˆæœŸåˆ†æ
        analysis = self._analyze_results(query, initial_results)
        summary = self._create_summary(query, analysis)

        additional_queries = []

        # åå¾©æ”¹å–„
        for iteration in range(max_iterations - 1):
            print(f"ğŸ”„ åå¾© {iteration + 1}: ãƒ¬ãƒãƒ¼ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­...")

            # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§è¿½åŠ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
            new_queries = self._generate_additional_queries(query, analysis, summary)

            # æ—¢å­˜ã®ã‚¯ã‚¨ãƒªã¨é‡è¤‡ã‚’é¿ã‘ã‚‹
            new_queries = [q for q in new_queries if q not in additional_queries and q != query]

            if not new_queries:
                print("è¿½åŠ ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                break

            additional_queries.extend(new_queries[:self.max_additional_queries])

            # è¿½åŠ æ¤œç´¢ã‚’å®Ÿè¡Œ
            for additional_query in new_queries[:self.max_additional_queries]:
                print(f"ğŸ” è¿½åŠ æ¤œç´¢: {additional_query}")
                additional_results = self.searcher.search(additional_query, num_results=self.additional_search_count)
                self.all_search_results.extend(additional_results)

            # å…¨çµæœã§å†åˆ†æ
            analysis = self._analyze_all_results(query, self.all_search_results)
            summary = self._create_summary(query, analysis)

            print(f"ğŸ“Š ç´¯ç©æ¤œç´¢çµæœ: {len(self.all_search_results)}ä»¶")

        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        final_report = self._create_final_report(query, analysis, summary)

        # å¼•ç”¨ã‚’æ•´ç†
        if self.citation_manager.auto_extract:
            self._organize_citations(analysis, final_report)

        return ResearchResult(
            query=query,
            search_results=self.all_search_results,
            analysis=analysis,
            summary=summary,
            citations=self.citation_manager.get_all_citations(),
            additional_queries=additional_queries,
            final_report=final_report
        )

    def _analyze_results(self, query: str, search_results: List[SearchResult]) -> str:
        """æ¤œç´¢çµæœã‚’åˆ†æ"""
        # ä»Šæ—¥ã®æ—¥ä»˜æƒ…å ±ã‚’æº–å‚™
        today_info = f"ä»Šæ—¥ã®æ—¥ä»˜: {self.today_date}"

        # å„æ¤œç´¢çµæœã«ç›¸å¯¾çš„ãªæ—¥ä»˜æƒ…å ±ã‚’è¿½åŠ 
        results_with_date_info = []
        for result in search_results:
            date_analysis = self._parse_date_info(result.date_info)
            relative_date = date_analysis.get("relative_info", "æ—¥ä»˜ä¸æ˜")
            is_future = date_analysis.get("is_future", False)
            is_recent = date_analysis.get("is_recent", False)

            date_status = ""
            if is_future:
                date_status = "ï¼ˆå°†æ¥ã®äºˆå®šï¼‰"
            elif is_recent:
                date_status = "ï¼ˆæœ€è¿‘ã®æƒ…å ±ï¼‰"
            elif date_analysis.get("is_valid", False):
                date_status = "ï¼ˆéå»ã®æƒ…å ±ï¼‰"

            # ä¿¡é ¼æ€§æƒ…å ±ã‚’è¿½åŠ 
            reliability_info = f"ä¿¡é ¼æ€§: {result.reliability_score:.2f}ï¼ˆ{result.source_type}ï¼‰"

            results_with_date_info.append(
                f"ã‚¿ã‚¤ãƒˆãƒ«: {result.title}\n"
                f"URL: {result.url}\n"
                f"å†…å®¹: {result.snippet}\n"
                f"æ—¥ä»˜æƒ…å ±: {result.date_info or 'ä¸æ˜'} â†’ {relative_date}{date_status}\n"
                f"{reliability_info}"
            )

        results_text = "\n\n".join(results_with_date_info)

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        prompt_template = self.config.get('prompts.analysis', """
ä»Šæ—¥ã®æ—¥ä»˜æƒ…å ±: {today_info}

ä»¥ä¸‹ã®æ¤œç´¢çµæœã®ã¿ã‚’åŸºã«ã€'{query}'ã«ã¤ã„ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚æ¤œç´¢çµæœã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã¯æ¨æ¸¬ã›ãšã€äº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚

æ¤œç´¢çµæœ:
{results_text}

åˆ†æã§ã¯ä»¥ä¸‹ã®ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
- æ¤œç´¢çµæœã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ä¸»è¦ãªäº‹å®Ÿï¼ˆé‡è¦åº¦é †ï¼‰
- æ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿ã‚„çµ±è¨ˆ
- æ¤œç´¢çµæœã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ç•°ãªã‚‹è¦–ç‚¹ã‚„æ„è¦‹
- å„æƒ…å ±ã®æ—¥ä»˜ï¼ˆæ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹å ´åˆï¼‰ã¨ä»Šæ—¥ã¨ã®é–¢ä¿‚
- å°†æ¥ã®äºˆå®šã‚„éå»ã®æƒ…å ±ã®åŒºåˆ¥
- æ¤œç´¢çµæœã‹ã‚‰ã¯ä¸æ˜ãªç‚¹ã‚„è¿½åŠ ã§èª¿ã¹ã‚‹ã¹ãç‚¹

é‡è¦ï¼šæ¤œç´¢çµæœã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã¯è¨˜è¼‰ã›ãšã€äº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
åˆ†æçµæœã‚’æ—¥æœ¬èªã§è©³ã—ãè¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
""")

        prompt = prompt_template.format(
            query=query,
            results_text=results_text,
            today_info=today_info
        )
        return self.model.generate(prompt)

    def _analyze_all_results(self, query: str, all_results: List[SearchResult]) -> str:
        """å…¨æ¤œç´¢çµæœã‚’åˆ†æ"""
        # ä»Šæ—¥ã®æ—¥ä»˜æƒ…å ±ã‚’æº–å‚™
        today_info = f"ä»Šæ—¥ã®æ—¥ä»˜: {self.today_date}"

        # å„æ¤œç´¢çµæœã«ç›¸å¯¾çš„ãªæ—¥ä»˜æƒ…å ±ã‚’è¿½åŠ 
        results_with_date_info = []
        for result in all_results:
            date_analysis = self._parse_date_info(result.date_info)
            relative_date = date_analysis.get("relative_info", "æ—¥ä»˜ä¸æ˜")
            is_future = date_analysis.get("is_future", False)
            is_recent = date_analysis.get("is_recent", False)

            date_status = ""
            if is_future:
                date_status = "ï¼ˆå°†æ¥ã®äºˆå®šï¼‰"
            elif is_recent:
                date_status = "ï¼ˆæœ€è¿‘ã®æƒ…å ±ï¼‰"
            elif date_analysis.get("is_valid", False):
                date_status = "ï¼ˆéå»ã®æƒ…å ±ï¼‰"

            results_with_date_info.append(
                f"ã‚¿ã‚¤ãƒˆãƒ«: {result.title}\n"
                f"URL: {result.url}\n"
                f"å†…å®¹: {result.snippet}\n"
                f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {result.search_query}\n"
                f"æ—¥ä»˜æƒ…å ±: {result.date_info or 'ä¸æ˜'} â†’ {relative_date}{date_status}"
            )

        results_text = "\n\n".join(results_with_date_info)

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        prompt_template = self.config.get('prompts.analysis_all', """
ä»Šæ—¥ã®æ—¥ä»˜æƒ…å ±: {today_info}

ä»¥ä¸‹ã®å…¨ã¦ã®æ¤œç´¢çµæœã®ã¿ã‚’åŸºã«ã€'{query}'ã«ã¤ã„ã¦åŒ…æ‹¬çš„ãªèª¿æŸ»çµæœã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æ¤œç´¢çµæœã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã¯æ¨æ¸¬ã›ãšã€äº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚

æ¤œç´¢çµæœ:
{results_text}

åˆ†æã§ã¯ä»¥ä¸‹ã®ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
- æ¤œç´¢çµæœã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ä¸»è¦ãªäº‹å®Ÿï¼ˆé‡è¦åº¦é †ï¼‰
- æ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿ã‚„çµ±è¨ˆ
- æ¤œç´¢çµæœã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ç•°ãªã‚‹è¦–ç‚¹ã‚„æ„è¦‹ã®æ¯”è¼ƒ
- å„æƒ…å ±ã®æ—¥ä»˜ï¼ˆæ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹å ´åˆï¼‰ã¨ä»Šæ—¥ã¨ã®é–¢ä¿‚
- å°†æ¥ã®äºˆå®šã‚„éå»ã®æƒ…å ±ã®åŒºåˆ¥
- ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æºã‹ã‚‰ã®æƒ…å ±
- æ¤œç´¢çµæœã‹ã‚‰ã¯ä¸æ˜ãªç‚¹ã‚„èª²é¡Œ

é‡è¦ï¼šæ¤œç´¢çµæœã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã¯è¨˜è¼‰ã›ãšã€äº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
åˆ†æçµæœã‚’æ—¥æœ¬èªã§è©³ã—ãè¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
""")

        prompt = prompt_template.format(
            query=query,
            results_text=results_text,
            today_info=today_info
        )
        return self.model.generate(prompt)

    def _create_summary(self, query: str, analysis: str) -> str:
        """è¦ç´„ã‚’ç”Ÿæˆ"""
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        prompt_template = self.config.get('prompts.summary', """
ä»¥ä¸‹ã®åˆ†æçµæœã®ã¿ã‚’åŸºã«ã€'{query}'ã«ã¤ã„ã¦ç°¡æ½”ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚åˆ†æçµæœã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã¯æ¨æ¸¬ã›ãšã€äº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚

åˆ†æçµæœ:
{analysis}

è¦ç´„ã§ã¯ä»¥ä¸‹ã®ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
- æ¤œç´¢çµæœã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æœ€ã‚‚é‡è¦ãªäº‹å®Ÿï¼ˆ3-5ç‚¹ï¼‰
- å„äº‹å®Ÿã®æ—¥ä»˜ï¼ˆåˆ†æçµæœã«å«ã¾ã‚Œã‚‹å ´åˆï¼‰
- çµè«–ï¼ˆæ¤œç´¢çµæœã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ç¯„å›²ã§ï¼‰

é‡è¦ï¼šåˆ†æçµæœã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã¯è¨˜è¼‰ã›ãšã€äº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
è¦ç´„ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
""")

        prompt = prompt_template.format(query=query, analysis=analysis)
        return self.model.generate(prompt)

    def _generate_additional_queries(self, original_query: str, analysis: str, summary: str) -> List[str]:
        """ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§è¿½åŠ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        # ä»Šæ—¥ã®æ—¥ä»˜æƒ…å ±ã‚’æº–å‚™
        today_info = f"ä»Šæ—¥ã®æ—¥ä»˜: {self.today_date}"

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        prompt_template = self.config.get('prompts.additional_queries', """
ä»Šæ—¥ã®æ—¥ä»˜æƒ…å ±: {today_info}

ä»¥ä¸‹ã®ç ”ç©¶çµæœã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ã€è¿½åŠ ã§èª¿ã¹ã‚‹ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

å…ƒã®ã‚¯ã‚¨ãƒª: {original_query}

åˆ†æçµæœ:
{analysis}

è¦ç´„:
{summary}

ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰è¿½åŠ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ææ¡ˆã—ã¦ãã ã•ã„ï¼š
1. åˆ†æã§è¨€åŠã•ã‚Œã¦ã„ã‚‹ãŒè©³ç´°ãŒä¸è¶³ã—ã¦ã„ã‚‹æ¦‚å¿µ
2. é–¢é€£ã™ã‚‹å°‚é–€ç”¨èªã‚„æŠ€è¡“
3. æ¯”è¼ƒå¯¾è±¡ã¨ãªã‚‹ä»–ã®äº‹ä¾‹ã‚„ãƒ‡ãƒ¼ã‚¿
4. æœ€æ–°ã®æƒ…å ±ã‚„çµ±è¨ˆ
5. åå¯¾æ„è¦‹ã‚„ç•°ãªã‚‹è¦–ç‚¹
6. å¤ã„æƒ…å ±ï¼ˆéå»ã®äºˆå®šã‚„ç™ºå£²æ—¥ï¼‰ã«ã¤ã„ã¦æœ€æ–°çŠ¶æ³ã‚’ç¢ºèªã™ã¹ãé …ç›®
7. å°†æ¥ã®äºˆå®šã«ã¤ã„ã¦æœ€æ–°ã®é€²æ—çŠ¶æ³ã‚’ç¢ºèªã™ã¹ãé …ç›®

ç‰¹ã«ã€è¨˜äº‹å†…ã§ç™ºå£²äºˆå®šæ—¥ã‚„ç™ºè¡¨äºˆå®šæ—¥ãŒä»Šæ—¥ã‚ˆã‚Šå‰ã®å ´åˆã¯ã€å®Ÿéš›ã®ç™ºå£²ãƒ»ç™ºè¡¨çŠ¶æ³ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€å°†æ¥ã®äºˆå®šã«ã¤ã„ã¦æœ€æ–°ã®é€²æ—ã‚„å¤‰æ›´ãŒãªã„ã‹ç¢ºèªã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚ææ¡ˆã—ã¦ãã ã•ã„ã€‚

å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯å…·ä½“çš„ã§æ¤œç´¢ã—ã‚„ã™ã„å½¢ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®å½¢å¼ã§æœ€å¤§5ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ææ¡ˆã—ã¦ãã ã•ã„ï¼š

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5

ç•ªå·ã‚„è¨˜å·ã¯ä»˜ã‘ãšã«ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
""")

        prompt = prompt_template.format(
            original_query=original_query,
            analysis=analysis,
            summary=summary,
            today_info=today_info
        )
        response = self.review_model.generate(prompt)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
        lines = response.strip().split('\n')
        queries = []
        for line in lines:
            line = line.strip()
            # ç©ºè¡Œã€ç•ªå·ã€è¨˜å·ã€èª¬æ˜æ–‡ã‚’é™¤å¤–
            if (line and
                not line.startswith(('#', '-', '*', '1.', '2.', '3.', '4.', '5.', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'è¿½åŠ ', 'ææ¡ˆ')) and
                len(line) > 2 and  # çŸ­ã™ãã‚‹è¡Œã‚’é™¤å¤–
                not line.endswith(':') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('ä»¥ä¸‹ã®') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('ç•ªå·ã‚„è¨˜å·') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('é‡è¦') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('åˆ†æã§è¨€åŠ') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('é–¢é€£ã™ã‚‹') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('æ¯”è¼ƒå¯¾è±¡') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('æœ€æ–°ã®') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('åå¯¾æ„è¦‹') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('ç‰¹ã«') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('ã¾ãŸ') and  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                not line.startswith('å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')):  # èª¬æ˜æ–‡ã‚’é™¤å¤–
                queries.append(line)

        print(f"ç”Ÿæˆã•ã‚ŒãŸè¿½åŠ ã‚¯ã‚¨ãƒª: {queries}")
        return queries[:5]  # æœ€å¤§5ã¤ã¾ã§

    def _create_final_report(self, query: str, analysis: str, summary: str) -> str:
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆçµ±åˆç‰ˆï¼‰"""
        # ä»Šæ—¥ã®æ—¥ä»˜æƒ…å ±ã‚’æº–å‚™
        today_info = f"ä»Šæ—¥ã®æ—¥ä»˜: {self.today_date}"

        # å¼•ç”¨æƒ…å ±ã‚’æº–å‚™
        citation_texts = []
        for i, result in enumerate(self.all_search_results, 1):
            date_analysis = self._parse_date_info(result.date_info)
            relative_date = date_analysis.get("relative_info", "æ—¥ä»˜ä¸æ˜")
            date_info = f"ï¼ˆ{result.date_info}ï¼‰" if result.date_info else ""
            citation_texts.append(f"[{i}] {result.title}{date_info} â†’ {relative_date}: {result.url}")

        citations_section = "\n".join(citation_texts)

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        prompt_template = self.config.get('prompts.final_report', """
ä»Šæ—¥ã®æ—¥ä»˜æƒ…å ±: {today_info}

ä»¥ä¸‹ã®åˆ†æçµæœã®ã¿ã‚’åŸºã«ã€'{query}'ã«ã¤ã„ã¦å­¦è¡“çš„ãªå“è³ªã®çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æ¤œç´¢çµæœã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã¯æ¨æ¸¬ã›ãšã€äº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚

åˆ†æçµæœ:
{analysis}

è¦ç´„:
{summary}

åˆ©ç”¨å¯èƒ½ãªæƒ…å ±æº:
{citations_section}

ãƒ¬ãƒãƒ¼ãƒˆã§ã¯ä»¥ä¸‹ã®æ§‹æˆã§ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ï¼ˆæ¤œç´¢çµæœã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æœ€ã‚‚é‡è¦ãªäº‹å®Ÿï¼‰
2. èƒŒæ™¯ã¨ç›®çš„
3. ä¸»è¦ãªç™ºè¦‹äº‹é …ï¼ˆæ¤œç´¢çµæœã«åŸºã¥ãè©³ç´°ãªåˆ†æï¼‰
4. ãƒ‡ãƒ¼ã‚¿ã¨çµ±è¨ˆï¼ˆæ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹æ•°å€¤æƒ…å ±ï¼‰
5. ç•°ãªã‚‹è¦–ç‚¹ã®æ¯”è¼ƒï¼ˆæ¤œç´¢çµæœã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ç¯„å›²ã§ï¼‰
6. çµè«–ã¨æ¨å¥¨äº‹é …ï¼ˆæ¤œç´¢çµæœã‹ã‚‰å°ãå‡ºã›ã‚‹ç¯„å›²ã§ï¼‰
7. ä»Šå¾Œã®ç ”ç©¶æ–¹å‘

å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§é©åˆ‡ãªå¼•ç”¨ã‚’è¡Œã„ã€ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æºã‚’æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
å„æƒ…å ±ã®æ—¥ä»˜ï¼ˆæ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹å ´åˆï¼‰ã¨ä»Šæ—¥ã¨ã®é–¢ä¿‚ã‚‚æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
å°†æ¥ã®äºˆå®šã‚„éå»ã®æƒ…å ±ã‚’åŒºåˆ¥ã—ã¦è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
å¼•ç”¨ã¯ [1], [2] ã®å½¢å¼ã§è¨˜è¼‰ã—ã€å¯¾å¿œã™ã‚‹æƒ…å ±æºã‚’æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚

é‡è¦ï¼šæ¤œç´¢çµæœã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã¯è¨˜è¼‰ã›ãšã€äº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
""")

        prompt = prompt_template.format(
            query=query,
            analysis=analysis,
            summary=summary,
            citations_section=citations_section,
            today_info=today_info
        )

        return self.model.generate(prompt)

    def _organize_citations(self, analysis: str, final_report: str):
        """åˆ†æçµæœã¨ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰å¼•ç”¨ã‚’æ•´ç†"""
        # åˆ†æçµæœã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦å¼•ç”¨ã‚’ä½œæˆ
        for result in self.all_search_results:
            # çµæœã®å†…å®¹ãŒåˆ†æã‚„ãƒ¬ãƒãƒ¼ãƒˆã§è¨€åŠã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if result.title in analysis or result.title in final_report:
                self.citation_manager.add_citation(result, result.snippet, relevance_score=1.0)

    def save_to_markdown(self, result: ResearchResult, filename: str = None) -> str:
        """çµæœã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        if not filename:
            filename = f"research_{result.query.replace(' ', '_')}.md"

        # å¼•ç”¨ãƒªãƒ³ã‚¯ã®è¾æ›¸ã‚’ä½œæˆ
        citation_links = {}
        for i, citation in enumerate(result.citations, 1):
            citation_links[f"[{i}]"] = f"[{i}]({citation.source_url})"

        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã«å¼•ç”¨ãƒªãƒ³ã‚¯ã‚’å·®ã—è¾¼ã¿
        final_report_with_links = result.final_report
        for citation_ref, link in citation_links.items():
            final_report_with_links = final_report_with_links.replace(citation_ref, link)

        content = f"""# Deep Research: {result.query}

## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
{result.summary}

## çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ
{final_report_with_links}

## è©³ç´°åˆ†æ
{result.analysis}

## æ¤œç´¢å±¥æ­´
### åˆæœŸæ¤œç´¢
- ã‚¯ã‚¨ãƒª: {result.query}

### è¿½åŠ æ¤œç´¢
"""

        # è¿½åŠ æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è¨˜è¼‰
        for i, query in enumerate(result.additional_queries, 1):
            content += f"- è¿½åŠ ã‚¯ã‚¨ãƒª {i}: {query}\n"

        content += f"""
## æ¤œç´¢çµæœï¼ˆ{len(result.search_results)}ä»¶ï¼‰
"""

        # æ¤œç´¢çµæœã‚’ã‚¯ã‚¨ãƒªåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        query_groups = {}
        for result_item in result.search_results:
            query = result_item.search_query
            if query not in query_groups:
                query_groups[query] = []
            query_groups[query].append(result_item)

        for query, results in query_groups.items():
            content += f"\n### æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\n"
            for i, search_result in enumerate(results, 1):
                date_info = f"ï¼ˆ{search_result.date_info}ï¼‰" if search_result.date_info else ""
                reliability_info = f"ä¿¡é ¼æ€§: {search_result.reliability_score:.2f}ï¼ˆ{search_result.source_type}ï¼‰"
                content += f"""
#### {i}. [{search_result.title}]({search_result.url}){date_info}
- **å†…å®¹**: {search_result.snippet}
- **{reliability_info}**

"""

        content += f"""
## å¼•ç”¨æ–‡çŒ®
"""

        # å¼•ç”¨æ–‡çŒ®ã‚’è¨˜è¼‰
        for i, citation in enumerate(result.citations, 1):
            date_info = f"ï¼ˆ{citation.date_info}ï¼‰" if citation.date_info else ""
            content += f"""
### [{i}] [{citation.source_title}]({citation.source_url}){date_info}
- **æ¤œç´¢ã‚¯ã‚¨ãƒª**: {citation.search_query}
- **é–¢é€£åº¦**: {citation.relevance_score:.2f}
- **å†…å®¹**: {citation.content}

"""

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"ğŸ“„ çµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        return filename

    def _parse_date_info(self, date_str: str) -> Dict[str, any]:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’è§£æã—ã¦ç›¸å¯¾çš„ãªæƒ…å ±ã‚’å–å¾—"""
        if not date_str:
            return {"is_valid": False, "relative_info": "æ—¥ä»˜ä¸æ˜"}

        try:
            # æ§˜ã€…ãªæ—¥ä»˜å½¢å¼ã‚’è§£æ
            date_patterns = [
                (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Yå¹´%mæœˆ%dæ—¥'),
                (r'(\d{4})-(\d{1,2})-(\d{1,2})', '%Y-%m-%d'),
                (r'(\d{4})/(\d{1,2})/(\d{1,2})', '%Y/%m/%d'),
                (r'(\d{1,2})æœˆ(\d{1,2})æ—¥', '%mæœˆ%dæ—¥'),
                (r'(\d{4})å¹´(\d{1,2})æœˆ', '%Yå¹´%mæœˆ'),
                (r'(\d{4})å¹´', '%Yå¹´'),
            ]

            for pattern, format_str in date_patterns:
                match = re.search(pattern, date_str)
                if match:
                    if format_str == '%Yå¹´%mæœˆ%dæ—¥':
                        year, month, day = match.groups()
                        parsed_date = datetime(int(year), int(month), int(day))
                    elif format_str == '%Y-%m-%d':
                        year, month, day = match.groups()
                        parsed_date = datetime(int(year), int(month), int(day))
                    elif format_str == '%Y/%m/%d':
                        year, month, day = match.groups()
                        parsed_date = datetime(int(year), int(month), int(day))
                    elif format_str == '%mæœˆ%dæ—¥':
                        month, day = match.groups()
                        # ä»Šå¹´ã®æ—¥ä»˜ã¨ã—ã¦æ‰±ã†
                        parsed_date = datetime(self.today_year, int(month), int(day))
                    elif format_str == '%Yå¹´%mæœˆ':
                        year, month = match.groups()
                        parsed_date = datetime(int(year), int(month), 1)
                    elif format_str == '%Yå¹´':
                        year = match.groups()[0]
                        parsed_date = datetime(int(year), 1, 1)

                    # ä»Šæ—¥ã¨ã®æ¯”è¼ƒ
                    today = datetime(self.today_year, self.today_month, self.today_day)
                    days_diff = (today - parsed_date).days

                    if days_diff > 0:
                        if days_diff < 7:
                            relative_info = f"{days_diff}æ—¥å‰"
                        elif days_diff < 30:
                            weeks = days_diff // 7
                            relative_info = f"{weeks}é€±é–“å‰"
                        elif days_diff < 365:
                            months = days_diff // 30
                            relative_info = f"{months}ãƒ¶æœˆå‰"
                        else:
                            years = days_diff // 365
                            relative_info = f"{years}å¹´å‰"
                    elif days_diff < 0:
                        if abs(days_diff) < 7:
                            relative_info = f"{abs(days_diff)}æ—¥å¾Œ"
                        elif abs(days_diff) < 30:
                            weeks = abs(days_diff) // 7
                            relative_info = f"{weeks}é€±é–“å¾Œ"
                        elif abs(days_diff) < 365:
                            months = abs(days_diff) // 30
                            relative_info = f"{months}ãƒ¶æœˆå¾Œ"
                        else:
                            years = abs(days_diff) // 365
                            relative_info = f"{years}å¹´å¾Œ"
                    else:
                        relative_info = "ä»Šæ—¥"

                    return {
                        "is_valid": True,
                        "parsed_date": parsed_date,
                        "relative_info": relative_info,
                        "days_diff": days_diff,
                        "is_future": days_diff < 0,
                        "is_recent": days_diff <= 30  # 30æ—¥ä»¥å†…ã‚’æœ€è¿‘ã¨ã™ã‚‹
                    }

            return {"is_valid": False, "relative_info": "æ—¥ä»˜å½¢å¼ãŒä¸æ˜"}

        except Exception as e:
            return {"is_valid": False, "relative_info": f"æ—¥ä»˜è§£æã‚¨ãƒ©ãƒ¼: {str(e)}"}

    def _sort_results_by_reliability(self, results: List[SearchResult]) -> List[SearchResult]:
        """ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦æ¤œç´¢çµæœã‚’ä¸¦ã³æ›¿ãˆ"""
        return sorted(results, key=lambda x: x.reliability_score, reverse=True)

    def _filter_results_by_reliability(self, results: List[SearchResult], threshold: float = 0.5) -> List[SearchResult]:
        """ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢ãŒé–¾å€¤ä»¥ä¸Šã®æ¤œç´¢çµæœã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        return [result for result in results if result.reliability_score >= threshold]

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ”¬ Deep Research Clone (æ”¹å–„ç‰ˆ)")
    print("=" * 50)

    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
    config_path = "config.yaml"
    if not os.path.exists(config_path):
        print(f"âš ï¸  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {config_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        print("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€config.example.yaml ã‚’ config.yaml ã«ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚")

    # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ
    print("ä½¿ç”¨ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("1. Ollama (ãƒ­ãƒ¼ã‚«ãƒ«)")
    print("2. OpenAI")
    print("3. Google Gemini")

    choice = input("é¸æŠ (1-3): ").strip()

    model_map = {
        "1": "ollama",
        "2": "openai",
        "3": "gemini"
    }

    model_type = model_map.get(choice, "ollama")

    try:
        # Deep Researchã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        researcher = DeepResearch(model_type, config_path)

        # ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›
        query = input("\nğŸ” ç ”ç©¶ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()

        if not query:
            print("ã‚¯ã‚¨ãƒªãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        # ç ”ç©¶ã‚’å®Ÿè¡Œ
        result = researcher.research(query)

        # æ¤œç´¢å¤±æ•—ã®å ´åˆ
        if result is None:
            print("\nâŒ ç ”ç©¶ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # çµæœã‚’è¡¨ç¤º
        print("\n" + "=" * 50)
        print("ğŸ“‹ ç ”ç©¶çµæœ")
        print("=" * 50)
        print(f"ã‚¯ã‚¨ãƒª: {result.query}")
        print(f"æ¤œç´¢çµæœæ•°: {len(result.search_results)}")
        print(f"è¿½åŠ æ¤œç´¢ã‚¯ã‚¨ãƒªæ•°: {len(result.additional_queries)}")
        print(f"å¼•ç”¨æ–‡çŒ®æ•°: {len(result.citations)}")
        print(f"è¦ç´„: {result.summary}")

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        filename = researcher.save_to_markdown(result)

        print(f"\nâœ… ç ”ç©¶å®Œäº†ï¼çµæœã¯ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()
